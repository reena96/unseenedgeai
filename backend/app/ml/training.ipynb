{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Skill Model Training Notebook\n",
    "\n",
    "This notebook provides an interactive interface for training, evaluating, and managing XGBoost models for skill inference.\n",
    "\n",
    "## Workflow:\n",
    "1. Data preparation and exploration\n",
    "2. Model training for each skill type\n",
    "3. Model evaluation against teacher ratings\n",
    "4. Model versioning and metadata management\n",
    "\n",
    "## Skills trained:\n",
    "- Empathy\n",
    "- Problem Solving\n",
    "- Self-Regulation\n",
    "- Resilience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports complete\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path.cwd().parent.parent))\n",
    "\n",
    "# Import our training modules=\n",
    "from app.ml.train_models import SkillModelTrainer\n",
    "from app.ml.evaluate_models import ModelEvaluator\n",
    "from app.ml.model_metadata import ModelRegistry, ModelMetadata\n",
    "from app.models.assessment import SkillType\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: ./data/training_data.csv\n",
      "Test data: ./data/test_data.csv\n",
      "Models directory: ./models\n",
      "Model version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "# File paths - UPDATE THESE\n",
    "TRAINING_DATA_PATH = \"./data/training_data.csv\"  # Path to training data\n",
    "TEST_DATA_PATH = \"./data/test_data.csv\"  # Path to test data with teacher ratings\n",
    "MODELS_DIR = \"./models\"  # Directory to save trained models\n",
    "MODEL_VERSION = \"1.0.0\"  # Version for this training run\n",
    "\n",
    "# Create models directory\n",
    "Path(MODELS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training data: {TRAINING_DATA_PATH}\")\n",
    "print(f\"Test data: {TEST_DATA_PATH}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Model version: {MODEL_VERSION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/training_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load training data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAINING_DATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTraining dataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraining_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mColumns: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(training_df.columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.14/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.14/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.14/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.14/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.14/lib/python/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './data/training_data.csv'"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "training_df = pd.read_csv(TRAINING_DATA_PATH)\n",
    "\n",
    "print(f\"Training dataset shape: {training_df.shape}\")\n",
    "print(f\"\\nColumns: {list(training_df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(training_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_data = training_df.isnull().sum()\n",
    "missing_data = missing_data[missing_data > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_data) > 0:\n",
    "    print(\"Missing values per column:\")\n",
    "    print(missing_data)\n",
    "else:\n",
    "    print(\"✓ No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display target variable distributions\n",
    "target_cols = ['empathy_score', 'problem_solving_score', 'self_regulation_score', 'resilience_score']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, col in enumerate(target_cols):\n",
    "    if col in training_df.columns:\n",
    "        axes[idx].hist(training_df[col], bins=20, edgecolor='black')\n",
    "        axes[idx].set_title(f'{col.replace(\"_\", \" \").title()} Distribution')\n",
    "        axes[idx].set_xlabel('Score')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        mean_val = training_df[col].mean()\n",
    "        axes[idx].axvline(mean_val, color='red', linestyle='--', label=f'Mean: {mean_val:.3f}')\n",
    "        axes[idx].legend()\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, f'{col} not found', ha='center', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\nTarget variable statistics:\")\n",
    "for col in target_cols:\n",
    "    if col in training_df.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(training_df[col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Train XGBoost models for each skill type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = SkillModelTrainer(\n",
    "    data_path=TRAINING_DATA_PATH,\n",
    "    models_dir=MODELS_DIR,\n",
    "    model_version=MODEL_VERSION\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")\n",
    "print(f\"Skills to train: {[skill.value for skill in trainer.skill_types]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all skill models\n",
    "print(\"Starting training...\\n\")\n",
    "trainer.train_all_skills()\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training Individual Skills (Optional)\n",
    "\n",
    "Train individual skills if needed for debugging or experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a specific skill (uncomment to use)\n",
    "# skill_type = SkillType.EMPATHY  # Change this to train different skills\n",
    "# \n",
    "# df = trainer.load_data()\n",
    "# X, y, feature_names = trainer.prepare_data(df, skill_type)\n",
    "# model, metrics = trainer.train_model(X, y, skill_type)\n",
    "# trainer.save_model(model, feature_names, skill_type, metrics, len(X))\n",
    "# \n",
    "# print(f\"\\n✓ {skill_type.value} model trained successfully\")\n",
    "# print(f\"Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Evaluate trained models against teacher ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(\n",
    "    models_dir=MODELS_DIR,\n",
    "    test_data_path=TEST_DATA_PATH\n",
    ")\n",
    "\n",
    "print(f\"✓ Evaluator initialized\")\n",
    "print(f\"Loaded models: {list(evaluator.models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "results = evaluator.evaluate_all_skills()\n",
    "\n",
    "# Save evaluation report\n",
    "report_path = Path(MODELS_DIR) / \"evaluation_report.json\"\n",
    "evaluator.save_evaluation_report(results, str(report_path))\n",
    "\n",
    "print(f\"\\n✓ Evaluation complete. Report saved to {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metrics for visualization\n",
    "skills = []\n",
    "pearson_scores = []\n",
    "spearman_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "r2_scores = []\n",
    "\n",
    "for skill, metrics in results.items():\n",
    "    if skill != 'summary' and isinstance(metrics, dict):\n",
    "        skills.append(skill.replace('_', ' ').title())\n",
    "        pearson_scores.append(metrics.get('pearson_r', 0))\n",
    "        spearman_scores.append(metrics.get('spearman_r', 0))\n",
    "        rmse_scores.append(metrics.get('rmse', 0))\n",
    "        mae_scores.append(metrics.get('mae', 0))\n",
    "        r2_scores.append(metrics.get('r2_score', 0))\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Correlation metrics\n",
    "x = np.arange(len(skills))\n",
    "width = 0.35\n",
    "axes[0, 0].bar(x - width/2, pearson_scores, width, label='Pearson r', alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, spearman_scores, width, label='Spearman r', alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Correlation Coefficient')\n",
    "axes[0, 0].set_title('Correlation with Teacher Ratings')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels(skills, rotation=45, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].axhline(y=0.7, color='r', linestyle='--', alpha=0.3, label='Target: 0.7')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Error metrics\n",
    "axes[0, 1].bar(x - width/2, rmse_scores, width, label='RMSE', alpha=0.8)\n",
    "axes[0, 1].bar(x + width/2, mae_scores, width, label='MAE', alpha=0.8)\n",
    "axes[0, 1].set_ylabel('Error')\n",
    "axes[0, 1].set_title('Prediction Error Metrics')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(skills, rotation=45, ha='right')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# R² scores\n",
    "axes[1, 0].bar(skills, r2_scores, color='green', alpha=0.7)\n",
    "axes[1, 0].set_ylabel('R² Score')\n",
    "axes[1, 0].set_title('R² Scores by Skill')\n",
    "axes[1, 0].set_xticklabels(skills, rotation=45, ha='right')\n",
    "axes[1, 0].axhline(y=0.5, color='r', linestyle='--', alpha=0.3, label='Target: 0.5')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Tolerance metrics\n",
    "tolerance_10 = [results[skill.lower().replace(' ', '_')].get('within_0.1', 0) * 100 for skill in skills]\n",
    "tolerance_15 = [results[skill.lower().replace(' ', '_')].get('within_0.15', 0) * 100 for skill in skills]\n",
    "tolerance_20 = [results[skill.lower().replace(' ', '_')].get('within_0.2', 0) * 100 for skill in skills]\n",
    "\n",
    "x = np.arange(len(skills))\n",
    "width = 0.25\n",
    "axes[1, 1].bar(x - width, tolerance_10, width, label='Within ±0.1', alpha=0.8)\n",
    "axes[1, 1].bar(x, tolerance_15, width, label='Within ±0.15', alpha=0.8)\n",
    "axes[1, 1].bar(x + width, tolerance_20, width, label='Within ±0.2', alpha=0.8)\n",
    "axes[1, 1].set_ylabel('Percentage (%)')\n",
    "axes[1, 1].set_title('Prediction Tolerance')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(skills, rotation=45, ha='right')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary metrics table\n",
    "summary_data = []\n",
    "for skill, metrics in results.items():\n",
    "    if skill != 'summary' and isinstance(metrics, dict):\n",
    "        summary_data.append({\n",
    "            'Skill': skill.replace('_', ' ').title(),\n",
    "            'Pearson r': f\"{metrics.get('pearson_r', 0):.3f}\",\n",
    "            'Spearman r': f\"{metrics.get('spearman_r', 0):.3f}\",\n",
    "            'RMSE': f\"{metrics.get('rmse', 0):.3f}\",\n",
    "            'MAE': f\"{metrics.get('mae', 0):.3f}\",\n",
    "            'R²': f\"{metrics.get('r2_score', 0):.3f}\",\n",
    "            'Within ±0.1': f\"{metrics.get('within_0.1', 0)*100:.1f}%\"\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Display overall summary\n",
    "if 'summary' in results:\n",
    "    print(\"\\nOverall Averages:\")\n",
    "    for metric, value in results['summary'].items():\n",
    "        print(f\"  {metric}: {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Registry and Metadata\n",
    "\n",
    "View and manage model versions and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize registry\n",
    "registry = ModelRegistry(models_dir=MODELS_DIR)\n",
    "\n",
    "# List all registered models\n",
    "models = registry.list_models()\n",
    "\n",
    "print(\"Registered Models:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for skill_type, metadata in models.items():\n",
    "    print(f\"\\n{skill_type.upper()}:\")\n",
    "    print(f\"  Version: {metadata.version}\")\n",
    "    print(f\"  Training Date: {metadata.training_date}\")\n",
    "    print(f\"  Model Type: {metadata.model_type}\")\n",
    "    print(f\"  Training Samples: {metadata.training_samples}\")\n",
    "    print(f\"  Feature Count: {metadata.feature_count}\")\n",
    "    print(f\"  Performance:\")\n",
    "    for metric, value in metadata.performance_metrics.items():\n",
    "        print(f\"    {metric}: {value:.4f}\")\n",
    "    print(f\"  Checksum: {metadata.model_checksum[:16]}...\")\n",
    "    \n",
    "    # Verify integrity\n",
    "    integrity_ok = registry.verify_model_integrity(skill_type)\n",
    "    status = \"✓ Valid\" if integrity_ok else \"✗ Corrupted\"\n",
    "    print(f\"  Integrity: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display hyperparameters\n",
    "print(\"\\nModel Hyperparameters:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for skill_type, metadata in models.items():\n",
    "    print(f\"\\n{skill_type.upper()}:\")\n",
    "    for param, value in metadata.hyperparameters.items():\n",
    "        print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Analyze feature importance for each skill\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, skill_type in enumerate([SkillType.EMPATHY, SkillType.PROBLEM_SOLVING, \n",
    "                                    SkillType.SELF_REGULATION, SkillType.RESILIENCE]):\n",
    "    model_path = Path(MODELS_DIR) / f\"{skill_type.value}_model.pkl\"\n",
    "    features_path = Path(MODELS_DIR) / f\"{skill_type.value}_features.pkl\"\n",
    "    \n",
    "    if model_path.exists() and features_path.exists():\n",
    "        model = joblib.load(model_path)\n",
    "        feature_names = joblib.load(features_path)\n",
    "        \n",
    "        # Get feature importance\n",
    "        importance = model.feature_importances_\n",
    "        \n",
    "        # Sort by importance\n",
    "        indices = np.argsort(importance)[::-1][:15]  # Top 15 features\n",
    "        \n",
    "        # Plot\n",
    "        axes[idx].barh(range(len(indices)), importance[indices], alpha=0.8)\n",
    "        axes[idx].set_yticks(range(len(indices)))\n",
    "        axes[idx].set_yticklabels([feature_names[i] for i in indices])\n",
    "        axes[idx].set_xlabel('Importance')\n",
    "        axes[idx].set_title(f'{skill_type.value.replace(\"_\", \" \").title()} - Top 15 Features')\n",
    "        axes[idx].invert_yaxis()\n",
    "        axes[idx].grid(axis='x', alpha=0.3)\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, f'Model not found for {skill_type.value}', \n",
    "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Make predictions on test data\n",
    "test_df = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for all skills\n",
    "predictions = {}\n",
    "\n",
    "for skill_type in [SkillType.EMPATHY, SkillType.PROBLEM_SOLVING, \n",
    "                   SkillType.SELF_REGULATION, SkillType.RESILIENCE]:\n",
    "    model_path = Path(MODELS_DIR) / f\"{skill_type.value}_model.pkl\"\n",
    "    features_path = Path(MODELS_DIR) / f\"{skill_type.value}_features.pkl\"\n",
    "    \n",
    "    if model_path.exists() and features_path.exists():\n",
    "        model = joblib.load(model_path)\n",
    "        feature_names = joblib.load(features_path)\n",
    "        \n",
    "        # Extract features\n",
    "        X = evaluator.extract_features(test_df, skill_type)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X)\n",
    "        y_pred = np.clip(y_pred, 0.0, 1.0)\n",
    "        \n",
    "        predictions[skill_type.value] = y_pred\n",
    "        test_df[f'{skill_type.value}_predicted'] = y_pred\n",
    "\n",
    "print(\"✓ Predictions complete\")\n",
    "print(f\"\\nPrediction columns added: {[f'{s}_predicted' for s in predictions.keys()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs ground truth\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "skill_types = [SkillType.EMPATHY, SkillType.PROBLEM_SOLVING, \n",
    "               SkillType.SELF_REGULATION, SkillType.RESILIENCE]\n",
    "\n",
    "for idx, skill_type in enumerate(skill_types):\n",
    "    teacher_col = f\"{skill_type.value}_teacher\"\n",
    "    pred_col = f\"{skill_type.value}_predicted\"\n",
    "    \n",
    "    if teacher_col in test_df.columns and pred_col in test_df.columns:\n",
    "        axes[idx].scatter(test_df[teacher_col], test_df[pred_col], alpha=0.5)\n",
    "        axes[idx].plot([0, 1], [0, 1], 'r--', alpha=0.5, label='Perfect prediction')\n",
    "        axes[idx].set_xlabel('Teacher Rating')\n",
    "        axes[idx].set_ylabel('Model Prediction')\n",
    "        axes[idx].set_title(f'{skill_type.value.replace(\"_\", \" \").title()}')\n",
    "        axes[idx].legend()\n",
    "        axes[idx].grid(alpha=0.3)\n",
    "        axes[idx].set_xlim([0, 1])\n",
    "        axes[idx].set_ylim([0, 1])\n",
    "    else:\n",
    "        axes[idx].text(0.5, 0.5, 'Data not available', \n",
    "                      ha='center', va='center', transform=axes[idx].transAxes)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "output_path = Path(MODELS_DIR) / \"test_predictions.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"✓ Predictions saved to {output_path}\")\n",
    "print(f\"\\nSaved columns: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Training workflow complete! This notebook has:\n",
    "\n",
    "1. ✓ Loaded and explored training data\n",
    "2. ✓ Trained XGBoost models for all skill types\n",
    "3. ✓ Evaluated models against teacher ratings\n",
    "4. ✓ Visualized model performance\n",
    "5. ✓ Analyzed feature importance\n",
    "6. ✓ Made predictions on test data\n",
    "7. ✓ Saved results and model metadata\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Review model performance metrics and identify areas for improvement\n",
    "- Experiment with hyperparameter tuning\n",
    "- Collect more training data to improve model accuracy\n",
    "- Deploy models to production environment\n",
    "- Set up monitoring for model performance drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
